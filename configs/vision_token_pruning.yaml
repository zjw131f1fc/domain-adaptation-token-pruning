# Vision Token Pruning with GAN 配置文件

global_settings:
  seed: 42
  device: "cuda"
  dtype: "float"  # 训练数据类型: "half" (float16) 或 "float" (float32)
  pytorch_cuda_alloc_conf: "expandable_segments:True"
  dataset_cache_dir: "/data/users/zjw/dataset_cache"
  hf_cache_dir: "/data/users/zjw/huggingface_cache"
  save_dir: "./outputs/checkpoints"
  log_dir: "./outputs/logs"
  study_name: "gan_vtp"  # Vision Token Pruning研究名称

trainer_settings:  # 训练框架设置
  type: "deep-learning"
  name: "basic-pytorch"
  dl_settings:
    epochs: 1
    batch_size: 10
    optimizers: # lr增大了10倍，不稳定的话需要改回去
      generator:
        type: "adam"
        lr: 2.0e-05
      discriminator:  # Discriminator优化器
        type: "adam"
        lr: 3.2e-03
      token_merger:  # Token Merger优化器（可以设置更高的学习率）
        type: "adam"
        lr: 1.0e-04
      layer_pruners:  # Layer Pruners优化器
        type: "adam"
        lr: 1.7e-04
    print_loss_every_batches: 20
    eval_every_batches: 500
    eval_max_samples: 500
    save_every_batches: 500
    save_every_epochs: 0
    optuna_eval_interval_batches: 40
    grad_clip_max_norm: 10.0

manager_settings:
  name: "basic"
  mode: "direct"  # null=单任务模式, "optuna"=超参数搜索, "batch_configs"=批量配置, "direct"=直接运行
  num_subtasks: 1
  available_gpus: [4,5,6]
  gpus_per_subtask: 3
  poll_interval: 5.0
  batch_configs_dir: null

search_settings:
  enable: false
  type: "optuna"
  n_trials: 200  # 减少trial数量（单worker串行执行）
  study_name: "attn_bias_search"
  pruner:
    type: "successive_halving"
    min_resource: 2  # 最少跑2个评估点才能被剪枝
    reduction_factor: 3
    min_early_stopping_rate: 0
  sampler:
    type: "tpe"
    n_startup_trials: 15  # TPE warmup：前10个trial随机采样
    multivariate: true  # 考虑参数间相关性
  params:
    # 搜索范围：基于最优参数上下两个数量级
    # 最优参数来源: best_params.json (trial)

    # Layer Pruners 学习率 (最优: 1.27e-4)
    trainer_settings.dl_settings.optimizers.layer_pruners.lr:
      type: "float"
      low: 1.0e-6   # 1.27e-4 / 100
      high: 1.0e-2  # 1.27e-4 * 100
      log: true

    # Discriminator 学习率 (最优: 1.30e-4)
    trainer_settings.dl_settings.optimizers.discriminator.lr:
      type: "float"
      low: 1.0e-6   # 1.30e-4 / 100
      high: 1.0e-2  # 1.30e-4 * 100
      log: true

    # Loss权重（基于最优参数上下两个数量级）
    # adv_loss_weight (最优: 29.1)
    method_settings.adv_loss_weight:
      type: "float"
      low: 0.3      # 29.1 / 100
      high: 3000.0  # 29.1 * 100
      log: true

    # task_loss_weight (最优: 3.87)
    method_settings.task_loss_weight:
      type: "float"
      low: 0.04     # 3.87 / 100
      high: 400.0   # 3.87 * 100
      log: true

    # sparsity_weight (最优: 0.212)
    method_settings.sparsity_weight:
      type: "float"
      low: 0.001    # 0.212 / 100
      high: 100.0    # 0.212 * 100
      log: true

    # token_count_loss_weight (最优: 7.02)
    method_settings.token_count_loss_weight:
      type: "float"
      low: 0.001    # 7.02 / 100
      high: 100.0   # 7.02 * 100
      log: true

    # binarization_loss_weight: 鼓励soft_mask接近0或1
    method_settings.binarization_loss_weight:
      type: "float"
      low: 0.001
      high: 100.0
      log: true

    # Discriminator配置（线性搜索，±0.1范围）
    # disc_reinit_prob (最优: 0.084)
    method_settings.disc_reinit_prob:
      type: "float"
      low: 0.0      # max(0, 0.084 - 0.1)
      high: 0.18    # 0.084 + 0.1
      log: false

    # disc_dropout (最优: 0.115)
    method_settings.disc_dropout:
      type: "float"
      low: 0.02     # max(0, 0.115 - 0.1)
      high: 0.22    # 0.115 + 0.1
      log: false

    # Attention Bias相关参数（新增）
    # use_attn_residual: 是否启用attention residual
    method_settings.use_attn_residual:
      type: "bool"

    # learnable_attn_weight: attention weight是否可学习
    method_settings.learnable_attn_weight:
      type: "bool"

    # ==================== Gumbel Softmax 相关参数 ====================
    # temperature: 初始temperature（控制soft/hard程度，典型值0.5-2.0）
    method_settings.temperature:
      type: "float"
      low: 0.5
      high: 2.0
      log: false

    # temperature_min: 最小temperature（annealing终点，典型值0.05-0.5）
    method_settings.temperature_min:
      type: "float"
      low: 0.05
      high: 0.5
      log: false

    # temperature_anneal_rate: Anneal比例（前X%步数进行annealing，0.3-0.8）
    method_settings.temperature_anneal_rate:
      type: "float"
      low: 0.1
      high: 0.8
      log: false

dataset_settings:  # 数据集配置
  name: "vqa-vqav2"  # 数据集名称
  split:
    train: 80000
    test: 2000
  category_priority:
    enable: false  # 禁用类别均衡，加快数据加载
    values:
      - train: "origin"
  fast_load_no_random: false  # 快速加载模式（Optuna搜索时加速）
  verbose_judge: false  # 是否打印judge详细信息（单条评估时的pred/ref对比）

config_settings:
  enable_dict_overrides: true
  enable_yaml_overrides: true
  log_config_on_load: true

backbone_settings:  # 多模态大模型骨架配置
  type: "mllm"  # 骨架类型
  name: "llava-1.5-7b"  # 模型名称
# name: "llava-1.5-7b" "qwen-2.5-3b"
  mllm_settings:
    device_map: "balanced"  # 显存分配策略（balanced/auto）
    max_text_tokens: 128  # 最大文本token数
    max_vision_tokens: 1600  # 最大视觉token数（减少降低显存占用）
    image_max_size: 1100  # 图像最大尺寸（减少降低vision token数）

    # ==================== Batch化相关配置 ====================
    enable_true_batch: true  # 是否启用真正的batch化处理（需要统一图片尺寸）
    unified_image_size: 336  # 统一的图片尺寸（正方形，如336x336）


method_settings:
  # ==================== Token Merger配置（新增） ====================
  enable_token_merger: false  # 是否启用token merger（true=启用, false=禁用）
  merger_type: "fixed_pooling"  # "simple" / "question_aware" / "fixed_pooling" (推荐V3)
  merge_ratio: 1.0  # 保留比例
  merger_dropout: 0.1  # Token Merger 的 dropout（防过拟合）
  merger_d_internal: 512  # Merger内部维度

  # ==================== Layer-wise Pruner配置（新增） ====================
  pruning_layers: [6, 12, 18]  # 要剪枝的LLM层索引
  pruner_d_internal: 1024  # Pruner内部维度
  pruner_num_heads: 4  # Cross-attention头数
  pruner_type: "cross_attention"  # "cross_attention" 或 "simple"
  pruner_dropout: 0  # Layer Pruner 的 dropout（防过拟合）

  # Attention Residual配置（新功能）
  use_attn_residual: true  # 是否启用attention residual（残差连接text→vision attention）
  attn_residual_weight: 0.5  # Residual权重（固定值或初始值）
  learnable_attn_weight: true  # Residual weight是否可学习（true=可学习参数，false=固定值）

  # ==================== 剪枝目标配置 ====================
  use_token_num_target: true  # true=使用绝对token数目标, false=使用稀疏度比例
  target_token_num: 200  # 目标保留的绝对token数（use_token_num_target=true时生效）
  target_sparsity: 0.3  # 目标稀疏度（use_token_num_target=false时生效，0.3表示剪掉30%）

  # ==================== Sparsity Loss配置 ====================
  sparsity_loss_only_on_excess: true  # true=只在超出目标时惩罚（强约束）, false=双向惩罚
  sparsity_weight: 9  # Sparsity约束loss权重（恒定值）
  sparsity_warmup_enable: false
  token_count_loss_weight: 0.08  # Token总数loss权重（弱惩罚，鼓励减少token）
  binarization_loss_weight: 0.81  # Binarization loss权重，鼓励soft_mask接近0或1

  # ==================== Temperature Annealing（新增） ====================
  temperature: 0.99  # 初始temperature
  temperature_min: 0.4  # 最小temperature
  temperature_anneal_rate: 0.4  # Anneal比例（前50%步数进行annealing）

  # ==================== Hard Pruning配置（评估时使用） ====================
  hard_pruning_threshold: 0.5  # Hard剪枝阈值（soft_mask > threshold则保留）

  # ==================== Generator配置 ====================
  gen_num_layers: 2
  gen_num_heads: 2
  gen_d_ff: 2048
  gen_use_pos_encoding: false
  gen_enable_token_bias: true
  gen_use_gumbel: true
  gen_temperature: 0.906
  gen_temperature_anneal: true
  gen_temperature_min: 0.237
  gen_temperature_anneal_rate: 0.543

  # ==================== Discriminator配置 ====================
  disc_num_layers: 3  # 使用的LLM隐层数量
  disc_d_d: 512  # 隐层维度
  # disc_dropout: 0.15  # [已废弃] Dropout比率（已被噪声替代）

  # 位置相关噪声：在pooling前对每个token添加不同强度的噪声
  disc_noise_scale_start: 0.05  # 前面token的噪声强度（信息不完整，加大噪声）
  disc_noise_scale_end: 0.01    # 后面token的噪声强度（信息完整，加小噪声）

  # 全局噪声（可选）：pooling后再添加的均匀噪声，通常设为0或很小
  disc_noise_scale: 0.0  # 全局噪声标准差（已有位置相关噪声，这里可设为0）

  disc_pool_start_weight: 0.3  # 加权融合时前面token的权重（信息不完整）
  disc_pool_end_weight: 1.0    # 加权融合时后面token的权重（信息完整）
  disc_target_layers: [-1, -3, -5]  # 目标LLM层索引
  disc_reinit_prob: 0.15  # 每个batch重初始化Discriminator的概率
  disc_use_spectral_norm: false  # 谱归一化（GAN稳定性）

  # ==================== 损失权重（动态调度） ====================
  # 余弦调度策略：训练初期优先任务性能，后期强化对抗训练

  # Task Loss权重（递减）
  task_loss_weight_start: 100.0  # 初始权重（高优先级，优先学习保留信息）
  task_loss_weight: 70.0  # 目标权重（warmup后的稳定值）

  # Adversarial Loss权重（递增）
  adv_loss_weight_start: 1.0  # 初始权重（低对抗，避免过早压制pruner）
  adv_loss_weight: 40  # 目标权重（warmup后强化对抗）

  # Entropy Loss权重
  entropy_weight: 12.0

  # Warmup配置
  loss_weight_warmup_ratio: 0.3  # 前30%步数进行warmup（0.0=关闭动态调度）

evaluation_settings:  # 评估配置
  eval_mode: ["origin", "hard"]  # origin=无剪枝, merge_only=只merge, soft=软剪枝, hard=硬剪枝
  eval_use_gumbel: false  # 评估时是否使用Gumbel Softmax (false=确定性推理，推荐)
