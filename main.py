"""Vision Token Pruning with GAN - ä¸»è®­ç»ƒè„šæœ¬

ä½¿ç”¨æ–°çš„Engineæ¶æ„å®ç°å¯¹æŠ—è®­ç»ƒçš„è§†è§‰tokenå‰ªæã€‚
"""

import torch
from typing import Dict, Any

from engine.configs.loader import load_config

# ===================== Managerå‡½æ•° =====================

def preload_fn(config: Dict) -> Dict[str, Any]:
    """é¢„åŠ è½½é‡é‡çº§èµ„æº"""
    from engine.datas.loader import load_dataset
    from engine.backbones.loader import load_backbone

    logger = config["logger"]
    logger.info("é¢„åŠ è½½Backboneå’ŒDataset...")
    
    backbone = load_backbone(config)
    
    # å†»ç»“Backboneå‚æ•°ï¼Œé¿å…æ¢¯åº¦ç´¯ç§¯å’Œæ˜¾å­˜æµªè´¹
    logger.info("å†»ç»“Backboneå‚æ•°...")
    if hasattr(backbone, "model"):
        for param in backbone.model.parameters():
            param.requires_grad = False
    
    dataset_bundle = load_dataset(config)
    
    return {
        "backbone": backbone,
        "dataset_bundle": dataset_bundle
    }


def run_fn(config: Dict, cache: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡Œè®­ç»ƒ"""
    from engine.trainers.loader import load_trainer
    from method import (
        LearnableTokenMerger,
        LearnableTokenMergerV2,
        LearnableTokenMergerV3,
        LayerSpecificPruner,
        Discriminator,
        train_step,
        train_step_batch,
        eval_step
    )

    logger = config["logger"]
    backbone = cache["backbone"]
    dataset_bundle = cache["dataset_bundle"]
    device = config.get("global_settings", {}).get("device")

    # æ ¹æ®é…ç½®é€‰æ‹©è®­ç»ƒå‡½æ•°
    enable_true_batch = config["backbone_settings"]["mllm_settings"].get("enable_true_batch", False)
    if enable_true_batch:
        logger.info("âš¡ BatchåŒ–æ¨¡å¼å·²å¯ç”¨ - ä½¿ç”¨ train_step_batch")
        train_step_fn = train_step_batch
    else:
        logger.info("ğŸ“ æ ‡å‡†æ¨¡å¼ - ä½¿ç”¨ train_step")
        train_step_fn = train_step

    # å°†dataset_bundleæ”¾å…¥configä¾›eval_stepä½¿ç”¨
    config["_dataset_bundle"] = dataset_bundle

    logger.info("åˆ›å»ºTrainer...")
    trainer = load_trainer(config, dataset_bundle)

    # è·å– backbone çš„è¾“å‡ºè®¾å¤‡
    backbone_output_device = getattr(backbone, 'output_device', backbone.device)
    model_dtype = torch.float16 if str(backbone_output_device).startswith('cuda') else torch.float32

    # ==================== åˆ›å»ºä¸¤é˜¶æ®µå‰ªææ¨¡å— ====================

    # 1. Token Merger
    logger.info("åˆ›å»ºToken Merger...")
    merger_type = config["method_settings"].get("merger_type", "simple")

    if merger_type == "fixed_pooling":
        # V3: å›ºå®šè¾“å‡ºMä¸ªtokensçš„å¯å­¦ä¹ æ± åŒ–ï¼ˆæ¨èï¼‰
        token_merger = LearnableTokenMergerV3(
            d_vision=config["backbone_settings"]["mllm_settings"]["vision_dim"],
            d_text=config["backbone_settings"]["mllm_settings"]["hidden_dim"],
            d_internal=config["method_settings"]["pruner_d_internal"],
            num_heads=config["method_settings"]["pruner_num_heads"],
            merge_ratio=config["method_settings"]["merge_ratio"],
            use_question=True  # é»˜è®¤å¯ç”¨question-aware
        ).to(device=device)
    elif merger_type == "question_aware":
        # V2: Question-aware with top-k
        token_merger = LearnableTokenMergerV2(
            d_vision=config["backbone_settings"]["mllm_settings"]["vision_dim"],
            d_text=config["backbone_settings"]["mllm_settings"]["hidden_dim"],
            d_internal=config["method_settings"]["pruner_d_internal"],
            num_heads=config["method_settings"]["pruner_num_heads"],
            merge_ratio=config["method_settings"]["merge_ratio"]
        ).to(device=device)
    else:
        # V1: Simple with top-k
        token_merger = LearnableTokenMerger(
            d_model=config["backbone_settings"]["mllm_settings"]["vision_dim"],
            num_heads=config["method_settings"]["pruner_num_heads"],
            merge_ratio=config["method_settings"]["merge_ratio"]
        ).to(device=device)

    # 2. Layer-Specific Pruners
    logger.info("åˆ›å»ºLayer-Specific Pruners...")
    layer_pruners = LayerSpecificPruner(
        d_model=config["backbone_settings"]["mllm_settings"]["hidden_dim"],
        d_text=config["backbone_settings"]["mllm_settings"]["hidden_dim"],
        layer_indices=config["method_settings"]["pruning_layers"],
        d_internal=config["method_settings"]["pruner_d_internal"],
        num_heads=config["method_settings"]["pruner_num_heads"],
        use_attn_residual=config["method_settings"].get("use_attn_residual", False),
        attn_residual_weight=config["method_settings"].get("attn_residual_weight", 0.5),
        learnable_attn_weight=config["method_settings"].get("learnable_attn_weight", False)
    ).to(device=device)

    # 3. Discriminator
    logger.info("åˆ›å»ºDiscriminator...")
    discriminator = Discriminator(
        d_model=config["backbone_settings"]["mllm_settings"]["hidden_dim"],
        num_layers=config["method_settings"]["disc_num_layers"],
        d_d=config["method_settings"]["disc_d_d"],
        dropout=config["method_settings"]["disc_dropout"],
        use_layer_norm=True,
        use_spectral_norm=config["method_settings"]["disc_use_spectral_norm"]
    ).to(device=device)

    # ==================== æ³¨å†Œæ¨¡å‹ ====================

    # trainer.register_model("token_merger", token_merger)
    trainer.register_model("layer_pruners", layer_pruners)
    trainer.register_model("discriminator", discriminator)
    trainer.register_model("backbone", backbone)

    # ==================== æ·»åŠ å‚æ•°ç»„ ====================
    # æ‹†åˆ†ä¸º3ä¸ªç‹¬ç«‹å‚æ•°ç»„ï¼Œæ”¯æŒä¸åŒå­¦ä¹ ç‡ï¼š
    # 1. token_merger: Tokenåˆå¹¶å™¨ï¼ˆè¾“å…¥é˜¶æ®µå‰ªæï¼‰
    # 2. layer_pruners: é€å±‚å‰ªæå™¨ï¼ˆLLMå†…éƒ¨å‰ªæï¼‰
    # 3. discriminator: åˆ¤åˆ«å™¨

    # trainer.add_param_group("token_merger", list(token_merger.parameters()))
    trainer.add_param_group("layer_pruners", list(layer_pruners.parameters()))
    trainer.add_param_group("discriminator", list(discriminator.parameters()))

    # ==================== åˆ›å»ºä¼˜åŒ–å™¨ ====================

    trainer.setup_optimizers()

    # ==================== æ³¨å†Œè®­ç»ƒå’Œè¯„ä¼°å‡½æ•° ====================

    trainer.register_train_step(train_step_fn)  # ä½¿ç”¨åŠ¨æ€é€‰æ‹©çš„è®­ç»ƒå‡½æ•°
    trainer.register_eval_step(eval_step)

    # ==================== æ‰§è¡Œè®­ç»ƒ ====================

    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info(f"è®­ç»ƒå‡½æ•°: {train_step_fn.__name__}")  # æ˜¾ç¤ºä½¿ç”¨çš„è®­ç»ƒå‡½æ•°
    logger.info(f"Token Mergerç±»å‹: {merger_type}")
    logger.info(f"Merge Ratio: {config['method_settings']['merge_ratio']}")
    logger.info(f"Pruning Layers: {config['method_settings']['pruning_layers']}")
    logger.info(f"Temperature: {config['method_settings']['temperature']} â†’ {config['method_settings']['temperature_min']}")

    result = trainer.run()

    return result


# ===================== ä¸»å‡½æ•° =====================

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = load_config(override_file="configs/vision_token_pruning.yaml")
    logger = config["logger"]

    from engine.managers.loader import load_manager

    logger.info("=" * 60)
    logger.info("Vision Token Pruning with GAN")
    logger.info("=" * 60)

    # åˆ›å»ºManager
    manager = load_manager(
        config=config,
        preload_fn=preload_fn,
        run_fn=run_fn,
        task_generator_fn=None,  # å•ä»»åŠ¡æ¨¡å¼
        result_handler_fn=None
    )
    
    # å¯åŠ¨è®­ç»ƒ
    manager.start()
    manager.wait()
    
    # è·å–ç»“æœæ‘˜è¦
    summary = manager.get_summary()
    logger.info("=" * 60)
    logger.info("è®­ç»ƒå®Œæˆ!")
    logger.info(f"æ€»ä»»åŠ¡æ•°: {summary['total_tasks']}")
    logger.info(f"å·²å®Œæˆ: {summary['completed']}")
    logger.info(f"å¤±è´¥: {summary['failed']}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
