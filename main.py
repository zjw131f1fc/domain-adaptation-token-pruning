"""Vision Token Pruning with GAN - ä¸»è®­ç»ƒè„šæœ¬

ä½¿ç”¨æ–°çš„Engineæ¶æ„å®ç°å¯¹æŠ—è®­ç»ƒçš„è§†è§‰tokenå‰ªæã€‚
"""

import torch
from typing import Dict, Any

from engine.configs.loader import load_config


# ===================== GPUæ˜¾å­˜ä¿æŠ¤ =====================

def reserve_gpu_memory(reserve_ratio=0.90):
    """
    é¢„åˆ†é…GPUæ˜¾å­˜ï¼Œé˜²æ­¢è¢«å…¶ä»–ç¨‹åºæŠ¢å 

    æ³¨æ„ï¼šå¦‚æœä½¿ç”¨äº†CUDA_VISIBLE_DEVICESï¼Œè¿™é‡Œä¼šè‡ªåŠ¨å¤„ç†å¯è§çš„GPU

    å‚æ•°:
        reserve_ratio: é¢„ç•™æ¯”ä¾‹ï¼ˆ0-1ï¼‰ï¼Œé»˜è®¤0.90è¡¨ç¤ºé¢„ç•™90%æ˜¾å­˜
    """
    print(f"ğŸ›¡ï¸  æ­£åœ¨é¢„åˆ†é…GPUæ˜¾å­˜ä»¥é˜²æ­¢è¢«æŠ¢å ...")

    reserved_tensors = []
    num_gpus = torch.cuda.device_count()  # è·å–å½“å‰å¯è§çš„GPUæ•°é‡

    if num_gpus == 0:
        print("   âš ï¸  æœªæ£€æµ‹åˆ°å¯ç”¨GPU")
        return reserved_tensors

    for device_id in range(num_gpus):
        try:
            # è·å–GPUæ€»æ˜¾å­˜
            total_memory = torch.cuda.get_device_properties(device_id).total_memory
            reserve_size = int(total_memory * reserve_ratio)

            # åˆ†é…ä¸€ä¸ªå¤§tensorå ä½æ˜¾å­˜
            # ä½¿ç”¨int8èŠ‚çœç©ºé—´ï¼ˆ1 byte per elementï¼‰
            num_elements = reserve_size // 1  # int8 = 1 byte
            dummy_tensor = torch.empty(num_elements, dtype=torch.int8, device=f'cuda:{device_id}')
            reserved_tensors.append(dummy_tensor)

            gpu_name = torch.cuda.get_device_name(device_id)
            print(f"   GPU {device_id} ({gpu_name}): å·²é¢„ç•™ {reserve_size / 1024**3:.2f} GB / {total_memory / 1024**3:.2f} GB")
        except Exception as e:
            print(f"   âš ï¸  GPU {device_id} é¢„åˆ†é…å¤±è´¥: {e}")

    return reserved_tensors

# ===================== Managerå‡½æ•° =====================

def preload_fn(config: Dict) -> Dict[str, Any]:
    """é¢„åŠ è½½é‡é‡çº§èµ„æº"""
    from engine.datas.loader import load_dataset
    from engine.backbones.loader import load_backbone

    logger = config["logger"]
    logger.info("é¢„åŠ è½½Backboneå’ŒDataset...")
    
    backbone = load_backbone(config)
    
    # å†»ç»“Backboneå‚æ•°ï¼Œé¿å…æ¢¯åº¦ç´¯ç§¯å’Œæ˜¾å­˜æµªè´¹
    logger.info("å†»ç»“Backboneå‚æ•°...")
    if hasattr(backbone, "model"):
        for param in backbone.model.parameters():
            param.requires_grad = False
    
    dataset_bundle = load_dataset(config)
    
    return {
        "backbone": backbone,
        "dataset_bundle": dataset_bundle
    }


def run_fn(config: Dict, cache: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡Œè®­ç»ƒ"""
    from engine.trainers.loader import load_trainer
    from method import (
        LearnableTokenMerger,
        LearnableTokenMergerV2,
        LayerSpecificPruner,
        Discriminator,
        train_step,
        eval_step
    )

    logger = config["logger"]
    backbone = cache["backbone"]
    dataset_bundle = cache["dataset_bundle"]
    device = config.get("global_settings", {}).get("device")

    # å°†dataset_bundleæ”¾å…¥configä¾›eval_stepä½¿ç”¨
    config["_dataset_bundle"] = dataset_bundle

    logger.info("åˆ›å»ºTrainer...")
    trainer = load_trainer(config, dataset_bundle)

    # è·å– backbone çš„è¾“å‡ºè®¾å¤‡
    backbone_output_device = getattr(backbone, 'output_device', backbone.device)
    model_dtype = torch.float16 if str(backbone_output_device).startswith('cuda') else torch.float32

    # ==================== åˆ›å»ºä¸¤é˜¶æ®µå‰ªææ¨¡å— ====================

    # 1. Token Merger
    logger.info("åˆ›å»ºToken Merger...")
    merger_type = config["method_settings"].get("merger_type", "simple")

    if merger_type == "question_aware":
        token_merger = LearnableTokenMergerV2(
            d_vision=config["backbone_settings"]["mllm_settings"]["vision_dim"],
            d_text=config["backbone_settings"]["mllm_settings"]["hidden_dim"],
            d_internal=config["method_settings"]["pruner_d_internal"],
            num_heads=config["method_settings"]["pruner_num_heads"],
            merge_ratio=config["method_settings"]["merge_ratio"]
        ).to(device=device)
    else:
        token_merger = LearnableTokenMerger(
            d_model=config["backbone_settings"]["mllm_settings"]["vision_dim"],
            num_heads=config["method_settings"]["pruner_num_heads"],
            merge_ratio=config["method_settings"]["merge_ratio"]
        ).to(device=device)

    # 2. Layer-Specific Pruners
    logger.info("åˆ›å»ºLayer-Specific Pruners...")
    layer_pruners = LayerSpecificPruner(
        d_model=config["backbone_settings"]["mllm_settings"]["hidden_dim"],
        d_text=config["backbone_settings"]["mllm_settings"]["hidden_dim"],
        layer_indices=config["method_settings"]["pruning_layers"],
        d_internal=config["method_settings"]["pruner_d_internal"],
        num_heads=config["method_settings"]["pruner_num_heads"]
    ).to(device=device)

    # 3. Discriminator
    logger.info("åˆ›å»ºDiscriminator...")
    discriminator = Discriminator(
        d_model=config["backbone_settings"]["mllm_settings"]["hidden_dim"],
        num_layers=config["method_settings"]["disc_num_layers"],
        d_d=config["method_settings"]["disc_d_d"],
        dropout=config["method_settings"]["disc_dropout"],
        use_layer_norm=True,
        use_spectral_norm=config["method_settings"]["disc_use_spectral_norm"]
    ).to(device=device)

    # ==================== æ³¨å†Œæ¨¡å‹ ====================

    trainer.register_model("token_merger", token_merger)
    trainer.register_model("layer_pruners", layer_pruners)
    trainer.register_model("discriminator", discriminator)
    trainer.register_model("backbone", backbone)

    # ==================== æ·»åŠ å‚æ•°ç»„ ====================
    # æ‹†åˆ†ä¸º3ä¸ªç‹¬ç«‹å‚æ•°ç»„ï¼Œæ”¯æŒä¸åŒå­¦ä¹ ç‡ï¼š
    # 1. token_merger: Tokenåˆå¹¶å™¨ï¼ˆè¾“å…¥é˜¶æ®µå‰ªæï¼‰
    # 2. layer_pruners: é€å±‚å‰ªæå™¨ï¼ˆLLMå†…éƒ¨å‰ªæï¼‰
    # 3. discriminator: åˆ¤åˆ«å™¨

    trainer.add_param_group("token_merger", list(token_merger.parameters()))
    trainer.add_param_group("layer_pruners", list(layer_pruners.parameters()))
    trainer.add_param_group("discriminator", list(discriminator.parameters()))

    # ==================== åˆ›å»ºä¼˜åŒ–å™¨ ====================

    trainer.setup_optimizers()

    # ==================== æ³¨å†Œè®­ç»ƒå’Œè¯„ä¼°å‡½æ•° ====================

    trainer.register_train_step(train_step)
    trainer.register_eval_step(eval_step)

    # ==================== æ‰§è¡Œè®­ç»ƒ ====================

    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info(f"Token Mergerç±»å‹: {merger_type}")
    logger.info(f"Merge Ratio: {config['method_settings']['merge_ratio']}")
    logger.info(f"Pruning Layers: {config['method_settings']['pruning_layers']}")
    logger.info(f"Temperature: {config['method_settings']['temperature']} â†’ {config['method_settings']['temperature_min']}")

    result = trainer.run()

    return result


# ===================== ä¸»å‡½æ•° =====================

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½é…ç½®
    config = load_config(override_file="configs/vision_token_pruning.yaml")
    logger = config["logger"]

    # ========== GPUæ˜¾å­˜ä¿æŠ¤ï¼ˆé˜²æ­¢è¢«å…¶ä»–ç¨‹åºæŠ¢å ï¼‰ ==========
    # è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯è§çš„GPUï¼ˆè€ƒè™‘CUDA_VISIBLE_DEVICESï¼‰
    if torch.cuda.is_available():
        logger.info("ğŸ›¡ï¸  å¯ç”¨GPUæ˜¾å­˜ä¿æŠ¤...")
        reserved_tensors = reserve_gpu_memory(reserve_ratio=0.90)
        # æ³¨æ„: reserved_tensorsä¸èƒ½è¢«åˆ é™¤ï¼Œå¦åˆ™æ˜¾å­˜ä¼šè¢«é‡Šæ”¾
        # è®­ç»ƒè¿‡ç¨‹ä¸­PyTorchä¼šè‡ªåŠ¨ç®¡ç†å®é™…ä½¿ç”¨çš„æ˜¾å­˜
    else:
        logger.info("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œè·³è¿‡æ˜¾å­˜ä¿æŠ¤")
        reserved_tensors = []

    from engine.managers.loader import load_manager

    logger.info("=" * 60)
    logger.info("Vision Token Pruning with GAN")
    logger.info("=" * 60)

    # åˆ›å»ºManager
    manager = load_manager(
        config=config,
        preload_fn=preload_fn,
        run_fn=run_fn,
        task_generator_fn=None,  # å•ä»»åŠ¡æ¨¡å¼
        result_handler_fn=None
    )
    
    # å¯åŠ¨è®­ç»ƒ
    manager.start()
    manager.wait()
    
    # è·å–ç»“æœæ‘˜è¦
    summary = manager.get_summary()
    logger.info("=" * 60)
    logger.info("è®­ç»ƒå®Œæˆ!")
    logger.info(f"æ€»ä»»åŠ¡æ•°: {summary['total_tasks']}")
    logger.info(f"å·²å®Œæˆ: {summary['completed']}")
    logger.info(f"å¤±è´¥: {summary['failed']}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
